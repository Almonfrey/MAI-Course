{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Almonfrey/MAI-Course/blob/main/class_8_underfitting_vs_overfitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YNLZnOVhsac"
      },
      "source": [
        "# UCI Heart Disease — MLP Underfitting vs Overfitting\n",
        "\n",
        "This notebook recreates and documents the supplied code that demonstrates underfitting and overfitting using the **UCI Heart Disease (Cleveland)** dataset and `sklearn`'s `MLPClassifier`.\n",
        "\n",
        "**Goals:**\n",
        "- Provide a clean, well-documented Jupyter-style notebook version of the code.\n",
        "- Split the work into logical parts (imports, data loading, cleaning, preprocessing, modeling, evaluation, plotting).\n",
        "- Explain *why* each step is done and what alternatives / caveats exist (important for medical data).\n",
        "\n",
        "---\n",
        "\n",
        "## Notes on usage\n",
        "- This is a *static* notebook (code cells are ready to run in a Jupyter environment). It does not execute here — run the cells in your environment (e.g., Jupyter Lab, Jupyter Notebook, VS Code) to get results and plots.\n",
        "- If you want, I can export this to a runnable `.ipynb` file next."
      ],
      "id": "7YNLZnOVhsac"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRfc8eHbhsae"
      },
      "source": [
        "## 1 — Imports and environment setup\n",
        "\n",
        "### Whats is done?\n",
        "- `pandas`, `numpy`: for data handling.\n",
        "- `sklearn` modules: for splitting, preprocessing, modeling, and metrics.\n",
        "- `matplotlib`: for plotting learning curves.\n",
        "- `warnings`: to silence convergence warnings (MLP often warns if not fully converged each epoch).\n",
        "- We use `warm_start=True` in `MLPClassifier` to manually control epochs and record accuracy at each step.\n",
        "\n",
        "### Why these imports?\n",
        "- **pandas / numpy**: core for reading and manipulating tabular data.\n",
        "- **train_test_split, cross_val_score, StratifiedKFold**: tools to measure generalization and to create train/test splits preserving class balance.\n",
        "- **Pipeline / ColumnTransformer**: ensure preprocessing is fitted only on training data — avoids leakage and keeps code tidy.\n",
        "- **SimpleImputer / StandardScaler / OneHotEncoder**: common preprocessing steps for missing values, scaling continuous features, and encoding categorical ones.\n",
        "- **MLPClassifier**: a feed-forward neural network. It's flexible and demonstrates capacity changes that produce under/overfitting behaviors.\n"
      ],
      "id": "KRfc8eHbhsae"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XSUouMZhsae"
      },
      "outputs": [],
      "source": [
        "# 1. Importing libraries\n",
        "# Standard data tools\n",
        "import pandas as pd            # dataframes (tabular data handling)\n",
        "import numpy as np             # numerical operations\n",
        "\n",
        "# Model selection and model building\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Preprocessing building-blocks\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Classifier and evaluation\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "6XSUouMZhsae"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04mnk2Bhhsaf"
      },
      "source": [
        "## 2 — Load and clean the UCI Heart Disease (Cleveland) dataset\n",
        "\n",
        "### Notes about the dataset\n",
        "- This dataset is widely used for classification examples. The `target` column in the original dataset is **0** for no disease and **1-4** for presence (different degrees). We convert to binary.\n",
        "- The raw file uses `?` as a placeholder for missing values in some columns (particularly `ca` and `thal`)."
      ],
      "id": "04mnk2Bhhsaf"
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load ORIGINAL UCI Heart Disease Cleveland dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
        "columns = [\n",
        "    \"age\",\"sex\",\"cp\",\"trestbps\",\"chol\",\"fbs\",\"restecg\",\"thalach\",\"exang\",\n",
        "    \"oldpeak\",\"slope\",\"ca\",\"thal\",\"target\"\n",
        "]\n",
        "\n",
        "df = pd.read_csv(url, names=columns)\n",
        "\n",
        "# Quick look (uncomment when running interactively)\n",
        "# display(df.head())\n",
        "# display(df.info())"
      ],
      "metadata": {
        "id": "yzx2NVb4kE6r"
      },
      "id": "yzx2NVb4kE6r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 — Cleaning procedures\n",
        "\n",
        "### What is done?\n",
        "- Load the original dataset from UCI.\n",
        "- Replace \"?\" with NaN so sklearn imputation can handle missing data.\n",
        "- Convert all columns to numeric: ensures correct dtypes for preprocessing.\n",
        "- Convert `target` to binary: original has 0 (no disease), 1-4 (presence of disease), so we map >0 to 1.\n",
        "\n",
        "### Detailed explanation\n",
        "- `df.replace(\"?\", np.nan)`: the dataset uses `?` where data is missing — converting to `np.nan` allows `pandas`/`sklearn` imputation utilities to work.\n",
        "- `pd.to_numeric(errors='coerce')`: forces columns to numeric dtype; any value that cannot be parsed becomes `NaN`. This normalizes heterogeneous types (strings + numbers).\n",
        "- After conversion we print missing counts so we can decide imputation strategy per column.\n",
        "- **Target conversion**: the dataset encodes disease severity with integers 0–4; turning it into a binary target (0 vs. 1) is typical when the task is disease *presence* detection.\n",
        "\n",
        "Caveat: for more nuanced risk modeling you could treat the problem as *ordinal* or *multiclass* rather than binary."
      ],
      "metadata": {
        "id": "Y6tZrNm_kDRx"
      },
      "id": "Y6tZrNm_kDRx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBNDF9JMhsaf"
      },
      "outputs": [],
      "source": [
        "# 3. Cleaning procedures\n",
        "# Replace \"?\" with np.nan\n",
        "df.replace(\"?\", np.nan, inplace=True)\n",
        "\n",
        "# Convert all columns to numeric, forcing errors to NaN\n",
        "for col in df.columns:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Check missing values\n",
        "print(\"Missing values per column:\\n\", df.isnull().sum())\n",
        "\n",
        "# Target variable: 0 (absence), 1-4 (presence). Convert to binary classification.\n",
        "# Using replace keeps the intention of the original code; an equivalent is: (df['target'] > 0).astype(int)\n",
        "df['target'] = df['target'].replace({1:1, 2:1, 3:1, 4:1})\n",
        "\n",
        "# A quick sanity check for target class balance (uncomment when running):\n",
        "# print(df['target'].value_counts(normalize=True))"
      ],
      "id": "xBNDF9JMhsaf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIpXN0pDhsaf"
      },
      "source": [
        "## 4 — Separate features and target\n",
        "\n",
        "**What is done?**\n",
        "- `X`: predictors; `y`: outcome (disease presence).\n",
        "- This separation is standard for supervised learning."
      ],
      "id": "LIpXN0pDhsaf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fXDI0SShsaf"
      },
      "outputs": [],
      "source": [
        "# 4. Separate features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']"
      ],
      "id": "0fXDI0SShsaf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCg5Xn-Ghsaf"
      },
      "source": [
        "## 5 — Identify numerical and categorical columns\n",
        "\n",
        "### What is done?\n",
        "- Some features are truly categorical (`cp`, `restecg`, `slope`, `ca`, `thal`).\n",
        "- Others are numerical.\n",
        "- Categorical features will be one-hot encoded; numerical features will be scaled.\n",
        "- This setup lets the neural network treat all features appropriately.\n",
        "\n",
        "### Detailed explanation\n",
        "\n",
        "The original code treats `cp`, `restecg`, `slope`, `ca`, and `thal` as categorical. That's reasonable because:\n",
        "- `cp` (chest pain type) has a few discrete codes.\n",
        "- `restecg` (resting electrocardiographic results) is discrete-coded.\n",
        "- `slope` is the slope of the peak exercise ST segment (discrete categories).\n",
        "- `ca` is the number of major vessels (0–3) but often stored with missing values and small integer domain — one-hot encoding is typical.\n",
        "- `thal` originally contains values referencing distinct categories (and `?` in raw data).\n",
        "\n",
        "We'll follow the same categorization for the pipeline below."
      ],
      "id": "fCg5Xn-Ghsaf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0HtXXgChsaf"
      },
      "outputs": [],
      "source": [
        "# 5. Identify numerical and categorical columns: 'ca' and 'thal' are categorical\n",
        "cat_cols = ['cp', 'restecg', 'slope', 'ca', 'thal']\n",
        "num_cols = [col for col in X.columns if col not in cat_cols]\n",
        "\n",
        "# For interactive inspection (uncomment to inspect):\n",
        "# for c in cat_cols:\n",
        "#     print(c, df[c].unique())"
      ],
      "id": "y0HtXXgChsaf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remarks on column types\n",
        "- Some columns (e.g., `ca`) are integer-valued but limited to a few categories; converting them to categorical avoids assuming ordinal/continuous relationships.\n",
        "- If you believe `ca` should be treated as ordinal (0 < 1 < 2 < 3), alternatives are `OrdinalEncoder` or keeping it numeric."
      ],
      "metadata": {
        "id": "TXxrdbfclVPl"
      },
      "id": "TXxrdbfclVPl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR3ol8GUhsag"
      },
      "source": [
        "## 6 — Train/test split\n",
        "### What is done?\n",
        "- 70% training, 30% test, stratified by `y` (preserves class balance).\n",
        "- Fixed random state for reproducibility.\n",
        "\n",
        "### Why stratify and random_state?\n",
        "- **Stratify** ensures that the proportion of positive/negative cases is similar in both train and test. This is especially important when classes are imbalanced.\n",
        "- **random_state** makes the split reproducible: the same code will produce the same split, useful for debugging and comparisons."
      ],
      "id": "vR3ol8GUhsag"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAPdwAyThsag"
      },
      "outputs": [],
      "source": [
        "# 6. Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=42\n",
        ")"
      ],
      "id": "fAPdwAyThsag"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwfY86rchsag"
      },
      "source": [
        "## 7 — Build preprocessing pipeline (imputation, scaling, encoding)\n",
        "\n",
        "### What is done?\n",
        "- Use `ColumnTransformer` to apply appropriate preprocessing to each column type.\n",
        "- **Numerical**: impute missing values (mean) and standardize (zero mean, unit variance).\n",
        "- **Categorical**: impute missing values (most frequent) and one-hot encode.\n",
        "- *Why pipeline?*: Ensures preprocessing is applied identically during fit and predict, and prevents data leakage."
      ],
      "id": "uwfY86rchsag"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqhMj6B8hsag"
      },
      "outputs": [],
      "source": [
        "# 7. Build preprocessing pipeline (imputation, scaling, encoding)\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), num_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]), cat_cols)\n",
        "])"
      ],
      "id": "bqhMj6B8hsag"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detailed explanation\n",
        "- **Numerical pipeline**:\n",
        "  - `SimpleImputer(strategy='mean')`: replaces missing numeric values with column mean. This is simple and robust for small amounts of missingness but may be suboptimal if missingness is informative.\n",
        "  - `StandardScaler()`: MLPs converge much faster when inputs are standardized (mean 0 and unit variance). Neural networks are sensitive to feature scaling.\n",
        "\n",
        "- **Categorical pipeline**:\n",
        "  - `SimpleImputer(strategy='most_frequent')`: fills missing categories with the most common category. Alternatives: a new category label like 'missing' or a separate missing indicator.\n",
        "  - `OneHotEncoder(handle_unknown='ignore')`: encode categories as binary columns. `handle_unknown='ignore'` prevents errors at inference time if unseen categories appear in test data.\n",
        "\n",
        "**Why use ColumnTransformer + Pipeline?**\n",
        "- Keeps the preprocessing steps encapsulated and ensures they are applied consistently during cross-validation and test set predictions.\n",
        "- Prevents data leakage: `fit` is only called on training data when using `cross_val_score` or `GridSearchCV` with the pipeline."
      ],
      "metadata": {
        "id": "4kiW4HmBoNeD"
      },
      "id": "4kiW4HmBoNeD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFsJU_L6hsag"
      },
      "source": [
        "## 8 — Define a Helper Function to Train an MLP, Record and Plot Accuracy per Epoch\n",
        "\n",
        "### What is done?\n",
        "- Use `warm_start=True` in `MLPClassifier` to iteratively train for 1 epoch at a time.\n",
        "- After each epoch, record train and test accuracy.\n",
        "- This allows us to plot how accuracy evolves during training."
      ],
      "id": "hFsJU_L6hsag"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-0sPVVHhsag"
      },
      "outputs": [],
      "source": [
        "# 8. Helper function to train, record and plot accuracy per epoch.\n",
        "def train_accuracy_curve(hidden_layer_sizes, model_name, **kwargs):\n",
        "    clf = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=1, warm_start=True, random_state=42, **kwargs)\n",
        "    pipe = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', clf)\n",
        "    ])\n",
        "    n_epochs = 100\n",
        "    train_acc = []\n",
        "    test_acc = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        pipe.fit(X_train, y_train)\n",
        "        train_acc.append(accuracy_score(y_train, pipe.predict(X_train)))\n",
        "        test_acc.append(accuracy_score(y_test, pipe.predict(X_test)))\n",
        "\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.plot(train_acc, label='Train Accuracy')\n",
        "    plt.plot(test_acc, label='Test Accuracy')\n",
        "    plt.title(f'Accuracy per Epoch — {model_name}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f'{model_name} — Final epoch: Train Accuracy: {train_acc[-1]:.3f}, Test Accuracy: {test_acc[-1]:.3f}')"
      ],
      "id": "E-0sPVVHhsag"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ9FFEmGhsag"
      },
      "source": [
        "## 9 — Run Helper Function to Train and Plot Learning Curves for all Models\n",
        "\n",
        "## What is done?\n",
        "\n",
        "- We run three models:\n",
        "  - **Underfit**: Very small network (2 neurons). Can't capture data patterns well.\n",
        "  - **Balanced**: Medium-sized network (30 neurons, `alpha=0.01`, `adam` solver, `learning_rate_init=0.001`). Tends to generalize better.\n",
        "  - **Overfit**: Large network (two layers of 100 neurons). Can memorize training data, but may not generalize.\n",
        "- Compare their learning curves to see underfitting/overfitting visually."
      ],
      "id": "zQ9FFEmGhsag"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gac7lFFchsag"
      },
      "outputs": [],
      "source": [
        "# 9. Run helper function\n",
        "# Underfit\n",
        "train_accuracy_curve((1,), 'Underfit MLP')\n",
        "\n",
        "# Balanced\n",
        "train_accuracy_curve((30,), 'Balanced MLP', alpha=0.01, learning_rate_init=0.001, solver='adam')\n",
        "\n",
        "# Overfit\n",
        "train_accuracy_curve((100, 100), 'Overfit MLP')"
      ],
      "id": "gac7lFFchsag"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ksskhg8Yhsag"
      },
      "source": [
        "## 9 — Interpreting the Curves & Next Steps\n",
        "\n",
        "**What and Why?**\n",
        "- **Underfit:** Train/test accuracy are both low and don't improve much — the model can't capture the complexity of the data.\n",
        "- **Balanced:** Both accuracies rise and plateau at similar values — good fit.\n",
        "- **Overfit:** Training accuracy may rise much higher than test accuracy, which plateaus or drops — the model memorizes training data.\n",
        "- If your balanced model plateaus at low test accuracy, try tuning `alpha`, `hidden_layer_sizes`, or use early stopping, or alternative models."
      ],
      "id": "Ksskhg8Yhsag"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}